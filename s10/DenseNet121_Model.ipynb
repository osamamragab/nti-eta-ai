{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5636151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Updated import\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5f5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57569e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Apple DS\"  # Replace with your actual image directory\n",
    "class_names = [\"Apple_BR\", \"Apple_CR\", \"Apple_HE\"]  # Replace with your actual class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e422b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize empty lists for storing image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each class directory\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(image_dir, class_name)\n",
    "    for image_path in os.listdir(class_dir):\n",
    "        image_paths.append(os.path.join(class_dir, image_path))\n",
    "        labels.append(class_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc6561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9863b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564bfcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 172\n",
      "Length of X_test: 54\n",
      "Length of X_val: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of X_train:\", len(X_train))\n",
    "print(\"Length of X_test:\", len(X_test))\n",
    "print(\"Length of X_val:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b6d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Train\"  # Replace with your desired path\n",
    "test_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Test\"  # Replace with your desired path\n",
    "val_dir =  r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Valid\"  # Replace with your desired path\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)  # Create directories if they don't exist\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "677209be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_val, y_val):\n",
    "    class_dir = os.path.join(val_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f7d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_train, y_train):\n",
    "    class_dir = os.path.join(train_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fd5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_test, y_test):\n",
    "    class_dir = os.path.join(test_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8247c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172 images belonging to 3 classes.\n",
      "Found 44 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 5  \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,            # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,      \n",
    "        zoom_range=0.2,    \n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True)  \n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "         rescale=1./255)       # normalize pixel values to [0,1]\n",
    "\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28a5c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laptop\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 415ms/step - accuracy: 0.2800 - loss: 11.6792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 387ms/step - accuracy: 0.3041 - loss: 11.6254 - val_accuracy: 0.4000 - val_loss: 11.2172 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 302ms/step - accuracy: 0.4223 - loss: 11.2012 - val_accuracy: 0.6316 - val_loss: 10.8703 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - accuracy: 0.5352 - loss: 10.9908 - val_accuracy: 0.8800 - val_loss: 10.5746 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 301ms/step - accuracy: 0.7149 - loss: 10.6217 - val_accuracy: 0.9474 - val_loss: 10.3267 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - accuracy: 0.6817 - loss: 10.5208 - val_accuracy: 0.9600 - val_loss: 10.1238 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 311ms/step - accuracy: 0.8446 - loss: 10.2658 - val_accuracy: 1.0000 - val_loss: 9.9570 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 322ms/step - accuracy: 0.8161 - loss: 10.1462 - val_accuracy: 0.9600 - val_loss: 9.8213 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 313ms/step - accuracy: 0.7789 - loss: 10.0595 - val_accuracy: 1.0000 - val_loss: 9.6132 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - accuracy: 0.8498 - loss: 9.8283 - val_accuracy: 0.9600 - val_loss: 9.5425 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 314ms/step - accuracy: 0.8529 - loss: 9.6897 - val_accuracy: 1.0000 - val_loss: 9.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - accuracy: 0.8548 - loss: 9.6113 - val_accuracy: 0.9600 - val_loss: 9.2951 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 330ms/step - accuracy: 0.8835 - loss: 9.5189 - val_accuracy: 0.9474 - val_loss: 9.1970 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 345ms/step - accuracy: 0.8609 - loss: 9.3544 - val_accuracy: 1.0000 - val_loss: 9.0593 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 347ms/step - accuracy: 0.8954 - loss: 9.2649 - val_accuracy: 1.0000 - val_loss: 8.9632 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 356ms/step - accuracy: 0.9177 - loss: 9.0739 - val_accuracy: 0.9600 - val_loss: 8.8872 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 343ms/step - accuracy: 0.9251 - loss: 8.9787 - val_accuracy: 1.0000 - val_loss: 8.8228 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 352ms/step - accuracy: 0.8589 - loss: 8.9720 - val_accuracy: 0.9600 - val_loss: 8.7151 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 354ms/step - accuracy: 0.9012 - loss: 8.8577 - val_accuracy: 1.0000 - val_loss: 8.5734 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 356ms/step - accuracy: 0.8826 - loss: 8.7608 - val_accuracy: 0.9600 - val_loss: 8.5277 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 357ms/step - accuracy: 0.9480 - loss: 8.5897 - val_accuracy: 1.0000 - val_loss: 8.3911 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Load the DenseNet121 model pretrained on ImageNet, specifying the input shape\n",
    "base_model = DenseNet121(weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5,\n",
    "    callbacks=[lr_scheduler],\n",
    "    class_weight={0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed77202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels:\n",
      "[1 1 0 1 0 2 2 2 2 2 1 2 1 2 2 2 0 0 1 1 0 0 1 0 1 2 0 1 0 1 2 0 0 2 0 0 2\n",
      " 2 2 1 1 0 2 1 1 1 1 0 2 1 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Use LabelEncoder to encode 'no' as 0 and 'yes' as 1\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Print the encoded labels\n",
    "print(\"Encoded Labels:\")\n",
    "print(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a4ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_aug = ImageDataGenerator(\n",
    "    # Rescale\n",
    "    rescale=1/255.0\n",
    ")\n",
    "\n",
    "# Read data from directory\n",
    "test_data = test_aug.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff316287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laptop\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 8.4520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.443034172058105, 1.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68f5178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 585ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.89183240e-02, 3.23723145e-02, 9.38709438e-01],\n",
       "       [8.91482592e-01, 6.29868126e-03, 1.02218725e-01],\n",
       "       [9.90735590e-01, 7.26958364e-03, 1.99473742e-03],\n",
       "       [8.17273092e-03, 1.63966343e-02, 9.75430667e-01],\n",
       "       [9.89793420e-01, 6.76004635e-03, 3.44661763e-03],\n",
       "       [5.71826957e-02, 4.59778532e-02, 8.96839380e-01],\n",
       "       [8.50723624e-01, 2.64149681e-02, 1.22861437e-01],\n",
       "       [9.61501479e-01, 2.44850107e-02, 1.40134981e-02],\n",
       "       [9.81241941e-01, 4.41999082e-03, 1.43380798e-02],\n",
       "       [1.76588036e-02, 9.64388192e-01, 1.79529972e-02],\n",
       "       [9.09194164e-03, 7.40374625e-02, 9.16870534e-01],\n",
       "       [5.59415435e-03, 9.93667781e-01, 7.38016504e-04],\n",
       "       [6.50941730e-01, 2.71261066e-01, 7.77971894e-02],\n",
       "       [3.37550929e-03, 9.52828825e-01, 4.37955819e-02],\n",
       "       [2.00529583e-02, 3.38292122e-02, 9.46117818e-01],\n",
       "       [7.54062116e-01, 7.44972527e-02, 1.71440586e-01],\n",
       "       [9.74881172e-01, 2.48148181e-02, 3.04056943e-04],\n",
       "       [1.03528805e-01, 1.56422257e-02, 8.80829036e-01],\n",
       "       [4.22229292e-03, 6.15029410e-03, 9.89627481e-01],\n",
       "       [5.07812062e-03, 9.93258834e-01, 1.66311685e-03],\n",
       "       [1.63685065e-02, 9.75723505e-01, 7.90805649e-03],\n",
       "       [1.55672366e-02, 4.93066907e-02, 9.35126066e-01],\n",
       "       [1.86674166e-02, 3.56440991e-02, 9.45688546e-01],\n",
       "       [1.55965546e-02, 9.78184283e-01, 6.21912256e-03],\n",
       "       [8.94176602e-01, 9.25285593e-02, 1.32948365e-02],\n",
       "       [7.05419620e-03, 9.91240919e-01, 1.70487526e-03],\n",
       "       [5.75207314e-03, 2.98612751e-02, 9.64386642e-01],\n",
       "       [9.70042229e-01, 2.58594640e-02, 4.09827894e-03],\n",
       "       [5.72803663e-03, 9.93590474e-01, 6.81564561e-04],\n",
       "       [5.55036217e-03, 2.83861030e-02, 9.66063559e-01],\n",
       "       [1.66078447e-03, 9.97966766e-01, 3.72509676e-04],\n",
       "       [6.80384506e-03, 9.92861569e-01, 3.34605546e-04],\n",
       "       [5.72904408e-01, 5.09302244e-02, 3.76165301e-01],\n",
       "       [1.49297854e-03, 9.93976831e-01, 4.53013042e-03],\n",
       "       [5.05718738e-02, 9.04931664e-01, 4.44964208e-02],\n",
       "       [8.46868992e-01, 7.71488575e-03, 1.45416215e-01],\n",
       "       [3.38609936e-03, 9.95396197e-01, 1.21769158e-03],\n",
       "       [1.04900142e-02, 5.78429066e-02, 9.31667030e-01],\n",
       "       [1.96823496e-02, 9.75675702e-01, 4.64191427e-03],\n",
       "       [4.53405865e-02, 1.26564894e-02, 9.42002952e-01],\n",
       "       [9.78598177e-01, 1.23734940e-02, 9.02837515e-03],\n",
       "       [9.66483176e-01, 2.11983435e-02, 1.23185636e-02],\n",
       "       [8.24424803e-01, 1.63889170e-01, 1.16860736e-02],\n",
       "       [1.78814419e-02, 2.21894514e-02, 9.59929049e-01],\n",
       "       [3.10557173e-03, 9.91083741e-01, 5.81067195e-03],\n",
       "       [7.64377229e-03, 9.85277593e-01, 7.07867090e-03],\n",
       "       [1.12328809e-02, 1.15774255e-02, 9.77189600e-01],\n",
       "       [5.49598992e-01, 2.40021925e-02, 4.26398784e-01],\n",
       "       [7.33001810e-03, 5.88023588e-02, 9.33867633e-01],\n",
       "       [1.57022465e-03, 9.97037292e-01, 1.39247067e-03],\n",
       "       [1.11540310e-01, 2.08388027e-02, 8.67620885e-01],\n",
       "       [5.13548125e-03, 9.94451106e-01, 4.13383998e-04],\n",
       "       [2.16694120e-02, 4.66479454e-03, 9.73665833e-01],\n",
       "       [9.92024422e-01, 4.24428610e-03, 3.73124098e-03]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming test_data is a tf.data.Dataset or similar that yields (input, label) pairs\n",
    "#y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
    "y_pred_prob = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # for multi-class classification\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439fdc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 247ms/step\n",
      "[[5.49598992e-01 2.40021925e-02 4.26398784e-01]\n",
      " [5.55036217e-03 2.83861030e-02 9.66063559e-01]\n",
      " [1.49297854e-03 9.93976831e-01 4.53013042e-03]\n",
      " [7.05419620e-03 9.91240919e-01 1.70487526e-03]\n",
      " [2.16694120e-02 4.66479454e-03 9.73665833e-01]\n",
      " [1.11540310e-01 2.08388027e-02 8.67620885e-01]\n",
      " [8.94176602e-01 9.25285593e-02 1.32948365e-02]\n",
      " [8.17273092e-03 1.63966343e-02 9.75430667e-01]\n",
      " [5.72904408e-01 5.09302244e-02 3.76165301e-01]\n",
      " [6.50941730e-01 2.71261066e-01 7.77971894e-02]\n",
      " [2.00529583e-02 3.38292122e-02 9.46117818e-01]\n",
      " [9.70042229e-01 2.58594640e-02 4.09827894e-03]\n",
      " [7.33001810e-03 5.88023588e-02 9.33867633e-01]\n",
      " [9.74881172e-01 2.48148181e-02 3.04056943e-04]\n",
      " [8.24424803e-01 1.63889170e-01 1.16860736e-02]\n",
      " [9.92024422e-01 4.24428610e-03 3.73124098e-03]\n",
      " [5.75207314e-03 2.98612751e-02 9.64386642e-01]\n",
      " [1.86674166e-02 3.56440991e-02 9.45688546e-01]\n",
      " [1.96823496e-02 9.75675702e-01 4.64191427e-03]\n",
      " [1.78814419e-02 2.21894514e-02 9.59929049e-01]\n",
      " [5.72803663e-03 9.93590474e-01 6.81564561e-04]\n",
      " [9.78598177e-01 1.23734940e-02 9.02837515e-03]\n",
      " [9.90735590e-01 7.26958364e-03 1.99473742e-03]\n",
      " [5.71826957e-02 4.59778532e-02 8.96839380e-01]\n",
      " [9.89793420e-01 6.76004635e-03 3.44661763e-03]\n",
      " [4.22229292e-03 6.15029410e-03 9.89627481e-01]\n",
      " [3.37550929e-03 9.52828825e-01 4.37955819e-02]\n",
      " [5.13548125e-03 9.94451106e-01 4.13383998e-04]\n",
      " [3.10557173e-03 9.91083741e-01 5.81067195e-03]\n",
      " [6.80384506e-03 9.92861569e-01 3.34605546e-04]\n",
      " [9.66483176e-01 2.11983435e-02 1.23185636e-02]\n",
      " [5.07812062e-03 9.93258834e-01 1.66311685e-03]\n",
      " [5.05718738e-02 9.04931664e-01 4.44964208e-02]\n",
      " [8.91482592e-01 6.29868126e-03 1.02218725e-01]\n",
      " [5.59415435e-03 9.93667781e-01 7.38016504e-04]\n",
      " [9.81241941e-01 4.41999082e-03 1.43380798e-02]\n",
      " [1.63685065e-02 9.75723505e-01 7.90805649e-03]\n",
      " [4.53405865e-02 1.26564894e-02 9.42002952e-01]\n",
      " [3.38609936e-03 9.95396197e-01 1.21769158e-03]\n",
      " [1.76588036e-02 9.64388192e-01 1.79529972e-02]\n",
      " [1.55672366e-02 4.93066907e-02 9.35126066e-01]\n",
      " [7.54062116e-01 7.44972527e-02 1.71440586e-01]\n",
      " [1.03528805e-01 1.56422257e-02 8.80829036e-01]\n",
      " [8.50723624e-01 2.64149681e-02 1.22861437e-01]\n",
      " [1.57022465e-03 9.97037292e-01 1.39247067e-03]\n",
      " [7.64377229e-03 9.85277593e-01 7.07867090e-03]\n",
      " [8.46868992e-01 7.71488575e-03 1.45416215e-01]\n",
      " [1.55965546e-02 9.78184283e-01 6.21912256e-03]\n",
      " [2.89183240e-02 3.23723145e-02 9.38709438e-01]\n",
      " [1.12328809e-02 1.15774255e-02 9.77189600e-01]\n",
      " [9.09194164e-03 7.40374625e-02 9.16870534e-01]\n",
      " [1.04900142e-02 5.78429066e-02 9.31667030e-01]\n",
      " [9.61501479e-01 2.44850107e-02 1.40134981e-02]\n",
      " [1.66078447e-03 9.97966766e-01 3.72509676e-04]]\n",
      "Time taken for prediction: 2.8436033725738525 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_data is a tf.data.Dataset or similar that yields (input, label) pairs\n",
    "\n",
    "# Start timing for the prediction process\n",
    "start_time = time.time()\n",
    "\n",
    "# Get predicted probabilities from the model\n",
    "y_pred_prob = model.predict(test_data)\n",
    "\n",
    "# End timing for the prediction process\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Convert probabilities to class predictions for multi-class classification\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print the predicted probabilities and the time taken\n",
    "print(y_pred_prob)\n",
    "print(f\"Time taken for prediction: {time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7532dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'BLAST1_001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11048\\3822613389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Load the image with target size matching the input shape of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Convert the image to a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BLAST1_001.jpg'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your image\n",
    "img_path = 'BLAST1_001.jpg'\n",
    "\n",
    "# Load the image with target size matching the input shape of the model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Start timing for the entire process (preprocessing + prediction)\n",
    "total_start_time = time.time()\n",
    "\n",
    "\n",
    "# Start timing for preprocessing\n",
    "preprocess_start_time = time.time()\n",
    "\n",
    "# Expand dimensions to match the expected input shape of the model (1, 400, 400, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Preprocess the image for MobileNetV2\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# End timing for preprocessing\n",
    "preprocess_end_time = time.time()\n",
    "preprocess_time_taken = preprocess_end_time - preprocess_start_time\n",
    "\n",
    "# Start timing for prediction\n",
    "start_time = time.time()\n",
    "\n",
    "# Make a prediction using the MobileNetV2 model\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# End timing for prediction\n",
    "end_time = time.time()\n",
    "prediction_time_taken = end_time - start_time\n",
    "\n",
    "# End timing for the entire process\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "\n",
    "# Print the predictions and time taken\n",
    "print(predictions)\n",
    "print(f\"Time taken for preprocessing: {preprocess_time_taken} seconds\")\n",
    "print(f\"Time taken for prediction: {prediction_time_taken} seconds\")\n",
    "print(f\"Total time taken (preprocessing + prediction): {total_time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc999a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9044164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255af42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
