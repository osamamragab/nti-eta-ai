{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b26c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Updated import\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e847cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48061e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Apple DS\"  # Replace with your actual image directory\n",
    "class_names = [\"Apple_BR\", \"Apple_CR\", \"Apple_HE\"]  # Replace with your actual class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c1f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize empty lists for storing image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each class directory\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(image_dir, class_name)\n",
    "    for image_path in os.listdir(class_dir):\n",
    "        image_paths.append(os.path.join(class_dir, image_path))\n",
    "        labels.append(class_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c77048",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels, \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc55766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 172\n",
      "Length of X_test: 54\n",
      "Length of X_val: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of X_train:\", len(X_train))\n",
    "print(\"Length of X_test:\", len(X_test))\n",
    "print(\"Length of X_val:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a520533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Train\"  # Replace with your desired path\n",
    "test_dir = r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Test\"  # Replace with your desired path\n",
    "val_dir =  r\"C:\\Users\\laptop\\AI Huawei\\26_8_2024\\Splitting DS\\Valid\"  # Replace with your desired path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137cf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_val, y_val):\n",
    "    class_dir = os.path.join(val_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_train, y_train):\n",
    "    class_dir = os.path.join(train_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07920540",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label in zip(X_test, y_test):\n",
    "    class_dir = os.path.join(test_dir, label)  # Construct class-specific subdirectory\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create class directory if it doesn't exist\n",
    "    shutil.copy(image_path, class_dir)  # Copy image to the class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20419762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 images belonging to 3 classes.\n",
      "Found 83 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 5  \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,            # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,      \n",
    "        zoom_range=0.2,    \n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True)  \n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "         rescale=1./255)       # normalize pixel values to [0,1]\n",
    "\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ba280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laptop\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 508ms/step - accuracy: 0.4373 - loss: 12.8237 - val_accuracy: 0.3600 - val_loss: 15.9661\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 433ms/step - accuracy: 0.5806 - loss: 8.6804 - val_accuracy: 0.2800 - val_loss: 10.9161\n",
      "Epoch 3/5\n",
      "\u001b[1m 8/20\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 407ms/step - accuracy: 0.8978 - loss: 6.0818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.8691 - loss: 6.0626 - val_accuracy: 0.6800 - val_loss: 6.1979\n",
      "Epoch 4/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 443ms/step - accuracy: 0.8717 - loss: 5.4916 - val_accuracy: 0.6250 - val_loss: 5.9538\n",
      "Epoch 5/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 436ms/step - accuracy: 0.7749 - loss: 4.4281 - val_accuracy: 0.6000 - val_loss: 5.1575\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Load the DenseNet121 model pretrained on ImageNet, specifying the input shape\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=20,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e716a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels:\n",
      "[0 1 0 1 2 1 2 0 1 2 2 2 2 0 1 2 2 1 1 0 2 2 0 0 2 0 0 2 1 0 1 0 1 2 2 0 1\n",
      " 0 1 1 0 1 2 0 0 2 1 2 1 2 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Use LabelEncoder to encode 'no' as 0 and 'yes' as 1\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Print the encoded labels\n",
    "print(\"Encoded Labels:\")\n",
    "print(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5bae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_aug = ImageDataGenerator(\n",
    "    # Rescale\n",
    "    rescale=1/255.0\n",
    ")\n",
    "\n",
    "# Read data from directory\n",
    "test_data = test_aug.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a019f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 2/21\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.7500 - loss: 5.4942 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laptop\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.6585 - loss: 5.1353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.07850980758667, 0.6274510025978088]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ccb54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.68146584e-02, 4.14609938e-04, 9.82770801e-01],\n",
       "       [3.50045204e-01, 8.80610850e-03, 6.41148627e-01],\n",
       "       [9.85287309e-01, 2.63015835e-07, 1.47124762e-02],\n",
       "       [4.98954467e-02, 7.31459249e-06, 9.50097263e-01],\n",
       "       [9.97483552e-01, 1.62891695e-07, 2.51641148e-03],\n",
       "       [9.86273110e-01, 1.68038156e-07, 1.37268035e-02],\n",
       "       [7.61562437e-02, 1.65182911e-03, 9.22191918e-01],\n",
       "       [8.74068916e-01, 5.09924860e-07, 1.25930533e-01],\n",
       "       [3.02891731e-01, 1.29908722e-06, 6.97106957e-01],\n",
       "       [3.30680758e-01, 8.26140959e-03, 6.61057830e-01],\n",
       "       [1.87573120e-01, 4.84271150e-04, 8.11942637e-01],\n",
       "       [5.47551997e-02, 1.99111091e-05, 9.45224881e-01],\n",
       "       [9.98083711e-01, 2.40170584e-06, 1.91389595e-03],\n",
       "       [7.27526486e-01, 2.33722525e-03, 2.70136267e-01],\n",
       "       [6.04534566e-01, 3.37156594e-01, 5.83088733e-02],\n",
       "       [9.96425331e-01, 1.26918277e-03, 2.30548321e-03],\n",
       "       [9.90909159e-01, 1.30384774e-06, 9.08957701e-03],\n",
       "       [3.22536498e-01, 1.28774082e-06, 6.77462220e-01],\n",
       "       [2.61142880e-01, 6.88967764e-01, 4.98893596e-02],\n",
       "       [6.45588100e-01, 3.06296806e-06, 3.54408830e-01],\n",
       "       [7.44319618e-01, 1.84488283e-06, 2.55678475e-01],\n",
       "       [9.98406947e-01, 4.94403721e-05, 1.54368114e-03],\n",
       "       [9.99825180e-01, 1.16439992e-06, 1.73620560e-04],\n",
       "       [9.60101426e-01, 2.22446573e-07, 3.98983471e-02],\n",
       "       [9.37275141e-02, 7.18975912e-07, 9.06271756e-01],\n",
       "       [1.62734881e-01, 1.63133302e-06, 8.37263525e-01],\n",
       "       [9.16772306e-01, 4.34528971e-07, 8.32272545e-02],\n",
       "       [6.54348671e-01, 2.15613014e-07, 3.45651120e-01],\n",
       "       [9.99732792e-01, 7.09080510e-08, 2.67023104e-04],\n",
       "       [4.40619618e-01, 2.87149633e-05, 5.59351683e-01],\n",
       "       [9.97982144e-01, 2.84509184e-07, 2.01759767e-03],\n",
       "       [9.78166908e-02, 8.05976859e-04, 9.01377380e-01],\n",
       "       [7.73429513e-01, 2.45975789e-05, 2.26545841e-01],\n",
       "       [9.59891856e-01, 3.92911286e-04, 3.97152752e-02],\n",
       "       [5.30180216e-01, 9.57622007e-02, 3.74057561e-01],\n",
       "       [8.61855090e-01, 2.95436550e-02, 1.08601309e-01],\n",
       "       [8.73633504e-01, 1.94465841e-07, 1.26366258e-01],\n",
       "       [6.10542715e-01, 1.32996170e-03, 3.88127297e-01],\n",
       "       [2.57534772e-01, 2.78568245e-03, 7.39679515e-01],\n",
       "       [1.20338999e-01, 9.63859975e-07, 8.79660070e-01],\n",
       "       [3.93314958e-01, 1.82169515e-07, 6.06684804e-01],\n",
       "       [1.62900776e-01, 4.61736954e-06, 8.37094605e-01],\n",
       "       [9.99522924e-01, 8.65465850e-08, 4.76925925e-04],\n",
       "       [9.99892354e-01, 9.15527437e-07, 1.06646563e-04],\n",
       "       [1.69106320e-01, 7.57374525e-01, 7.35191777e-02],\n",
       "       [3.04516941e-01, 1.57591398e-06, 6.95481539e-01],\n",
       "       [7.75120676e-01, 6.26110022e-07, 2.24878639e-01],\n",
       "       [9.64772925e-02, 2.18921719e-04, 9.03303742e-01],\n",
       "       [9.34089839e-01, 2.92170995e-07, 6.59097582e-02],\n",
       "       [9.60694313e-01, 5.28533178e-07, 3.93051244e-02],\n",
       "       [9.76518333e-01, 1.72875716e-07, 2.34814901e-02],\n",
       "       [9.39856350e-01, 1.77946895e-05, 6.01258092e-02],\n",
       "       [9.41908300e-01, 4.19465039e-07, 5.80912456e-02],\n",
       "       [9.29657876e-01, 5.25546819e-03, 6.50866181e-02],\n",
       "       [9.96940374e-01, 2.91780708e-07, 3.05938045e-03],\n",
       "       [9.59113479e-01, 4.01859580e-07, 4.08860110e-02],\n",
       "       [1.87301025e-01, 3.08459894e-05, 8.12668204e-01],\n",
       "       [9.98196900e-01, 3.56797926e-07, 1.80271000e-03],\n",
       "       [5.04084527e-01, 1.23742726e-02, 4.83541191e-01],\n",
       "       [1.04612447e-01, 1.11529520e-02, 8.84234667e-01],\n",
       "       [7.61280507e-02, 2.89341187e-06, 9.23869014e-01],\n",
       "       [1.15067326e-01, 7.80807078e-01, 1.04125567e-01],\n",
       "       [1.40850982e-02, 4.82263975e-04, 9.85432684e-01],\n",
       "       [6.24101877e-01, 3.67004603e-01, 8.89348052e-03],\n",
       "       [4.55229312e-01, 1.67827879e-04, 5.44602871e-01],\n",
       "       [9.66362596e-01, 1.06579728e-06, 3.36363614e-02],\n",
       "       [3.12562764e-01, 1.09308951e-06, 6.87436163e-01],\n",
       "       [9.76065755e-01, 1.18012977e-04, 2.38162819e-02],\n",
       "       [2.92645395e-01, 1.96719840e-01, 5.10634780e-01],\n",
       "       [9.98919010e-01, 2.88495556e-07, 1.08083128e-03],\n",
       "       [9.98039424e-01, 1.56298086e-07, 1.96047686e-03],\n",
       "       [1.02727331e-01, 2.27901299e-04, 8.97044718e-01],\n",
       "       [2.49360159e-01, 1.47477794e-03, 7.49165118e-01],\n",
       "       [8.98210406e-01, 9.46945697e-03, 9.23201218e-02],\n",
       "       [6.22482538e-01, 3.84296982e-07, 3.77517104e-01],\n",
       "       [9.74282026e-01, 1.60100230e-03, 2.41169352e-02],\n",
       "       [3.82257342e-01, 4.62885907e-07, 6.17742181e-01],\n",
       "       [2.84367651e-02, 8.89435978e-05, 9.71474349e-01],\n",
       "       [4.99513447e-01, 1.75792829e-03, 4.98728663e-01],\n",
       "       [4.93361741e-01, 4.90637660e-07, 5.06637752e-01],\n",
       "       [9.99127090e-01, 9.13622789e-05, 7.81513983e-04],\n",
       "       [9.45964396e-01, 1.22468535e-07, 5.40354364e-02],\n",
       "       [9.68984365e-01, 2.38081100e-07, 3.10153626e-02],\n",
       "       [9.97382939e-01, 8.93325875e-08, 2.61688442e-03],\n",
       "       [5.14839925e-02, 6.65199058e-03, 9.41864073e-01],\n",
       "       [7.47244060e-01, 1.27674296e-01, 1.25081629e-01],\n",
       "       [4.60840166e-01, 6.89494414e-07, 5.39159179e-01],\n",
       "       [1.14919260e-01, 1.57671398e-04, 8.84922981e-01],\n",
       "       [8.88779938e-01, 2.37220502e-06, 1.11217789e-01],\n",
       "       [9.97724712e-01, 2.26415135e-03, 1.11541985e-05],\n",
       "       [5.72194457e-01, 2.14967571e-07, 4.27805394e-01],\n",
       "       [8.03627133e-01, 2.71293195e-03, 1.93659887e-01],\n",
       "       [5.39003909e-01, 4.30851011e-03, 4.56687629e-01],\n",
       "       [4.98318970e-01, 1.06008702e-06, 5.01679957e-01],\n",
       "       [8.23865891e-01, 1.59210801e-01, 1.69233903e-02],\n",
       "       [2.86990523e-01, 8.86731650e-05, 7.12920845e-01],\n",
       "       [2.59500027e-01, 4.68035296e-07, 7.40499556e-01],\n",
       "       [1.11714385e-01, 3.73574323e-07, 8.88285160e-01],\n",
       "       [4.09098379e-02, 8.31282232e-04, 9.58258927e-01],\n",
       "       [8.21924627e-01, 4.87939701e-07, 1.78074926e-01],\n",
       "       [9.79059219e-01, 1.15948183e-07, 2.09406968e-02],\n",
       "       [5.24073914e-02, 4.04492457e-05, 9.47552145e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming test_data is a tf.data.Dataset or similar that yields (input, label) pairs\n",
    "#y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
    "y_pred_prob = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # for multi-class classification\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c13b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "[[9.9518484e-01 4.3396721e-08 4.8151258e-03]]\n",
      "Time taken for preprocessing: 0.0 seconds\n",
      "Time taken for prediction: 1.4396450519561768 seconds\n",
      "Total time taken (preprocessing + prediction): 1.4396450519561768 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your image\n",
    "img_path = '00e909aa-e3ae-4558-9961-336bb0f35db3___JR_FrgE.S 8593.jpg'\n",
    "\n",
    "# Load the image with target size matching the input shape of the model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Start timing for the entire process (preprocessing + prediction)\n",
    "total_start_time = time.time()\n",
    "\n",
    "\n",
    "# Start timing for preprocessing\n",
    "preprocess_start_time = time.time()\n",
    "\n",
    "# Expand dimensions to match the expected input shape of the model (1, 400, 400, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Preprocess the image for MobileNetV2\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# End timing for preprocessing\n",
    "preprocess_end_time = time.time()\n",
    "preprocess_time_taken = preprocess_end_time - preprocess_start_time\n",
    "\n",
    "# Start timing for prediction\n",
    "start_time = time.time()\n",
    "\n",
    "# Make a prediction using the MobileNetV2 model\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# End timing for prediction\n",
    "end_time = time.time()\n",
    "prediction_time_taken = end_time - start_time\n",
    "\n",
    "# End timing for the entire process\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "\n",
    "# Print the predictions and time taken\n",
    "print(predictions)\n",
    "print(f\"Time taken for preprocessing: {preprocess_time_taken} seconds\")\n",
    "print(f\"Time taken for prediction: {prediction_time_taken} seconds\")\n",
    "print(f\"Total time taken (preprocessing + prediction): {total_time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191bd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc38801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
